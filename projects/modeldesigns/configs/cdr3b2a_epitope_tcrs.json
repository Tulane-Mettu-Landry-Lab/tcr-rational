{
  "rules": {
    "0": {
      "rule": "template",
      "template": "{TRA}",
      "tokenizer": "aminoacid",
      "prefix": "tra",
      "mapping": {}
    },
    "1": {
      "rule": "template",
      "template": "{TRB}",
      "tokenizer": "aminoacid",
      "prefix": "trb",
      "mapping": {}
    },
    "3": {
      "rule": "template",
      "template": "{CDR2A}",
      "tokenizer": "aminoacid",
      "prefix": "cdr2a",
      "mapping": {}
    },
    "7": {
      "rule": "template",
      "template": "{CDR3B}",
      "tokenizer": "aminoacid",
      "prefix": "cdr3b",
      "mapping": {}
    },
    "8": {
      "rule": "template",
      "template": "{epitope}",
      "tokenizer": "aminoacid",
      "prefix": "epitope",
      "mapping": {}
    },
    "9": {
      "rule": "forward",
      "keys": [
        "Label"
      ],
      "prefix": null,
      "mapping": {
        "Label": "binder_label"
      }
    }
  },
  "collator": {
    "mlm": true,
    "mlm_probability": 0.5,
    "mask_replace_prob": 1.0,
    "random_replace_prob": 0.0,
    "pad_to_multiple_of": null,
    "seed": null
  },
  "tokenizers": {
    "0": {
      "name": "aminoacid",
      "path": "tokenizers/aminoacid",
      "single": "<CLS> $A <EOS>",
      "pair": "<CLS> $A <SEP> $B:1 <EOS>:1"
    }
  },
  "loss": {
    "header_binder": {
      "mix_weight": 0.5,
      "loss": "cross_entropy",
      "act_func": "softmax",
      "reduction": "mean",
      "label_smoothing": 0.0
    },
    "mlm_epitope": {
      "mix_weight": 0.05,
      "loss": "forward",
      "act_func": null
    },
    "mlm_cdr2a": {
      "mix_weight": 0.05,
      "loss": "forward",
      "act_func": null
    },
    "mlm_cdr3b": {
      "mix_weight": 0.05,
      "loss": "forward",
      "act_func": null
    },
    "mlm_tra": {
      "mix_weight": 0.05,
      "loss": "forward",
      "act_func": null
    },
    "mlm_trb": {
      "mix_weight": 0.05,
      "loss": "forward",
      "act_func": null
    },
    "decoder_epitope": {
      "mix_weight": 1.0,
      "loss": "forward",
      "act_func": null
    },
    "decoder_tra": {
      "mix_weight": 1.0,
      "loss": "forward",
      "act_func": null
    },
    "decoder_trb": {
      "mix_weight": 1.0,
      "loss": "forward",
      "act_func": null
    }
  },
  "model": {
    "epitope": {
      "prefix": "epitope",
      "model": "BertForMaskedLM",
      "tokenizer": "aminoacid",
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 50,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "cdr2a": {
      "prefix": "cdr2a",
      "model": "BertForMaskedLM",
      "tokenizer": "aminoacid",
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 50,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "cdr3b": {
      "prefix": "cdr3b",
      "model": "BertForMaskedLM",
      "tokenizer": "aminoacid",
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 50,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "tra": {
      "prefix": "tra",
      "model": "BertForMaskedLM",
      "tokenizer": "aminoacid",
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 350,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "trb": {
      "prefix": "trb",
      "model": "BertForMaskedLM",
      "tokenizer": "aminoacid",
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 350,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "decoder_trb": {
      "prefix": "trb",
      "model": "BertModel",
      "tokenizer": "aminoacid",
      "hidden_state_keys":["epitope", "cdr2a", "cdr3b"],
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 350,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "decoder_tra": {
      "prefix": "tra",
      "model": "BertModel",
      "tokenizer": "aminoacid",
      "hidden_state_keys":["cdr2a", "cdr3b"],
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 350,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "decoder_epitope": {
      "prefix": "epitope",
      "model": "BertModel",
      "tokenizer": "aminoacid",
      "hidden_state_keys":["cdr2a", "cdr3b"],
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 50,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "header_binder": {
      "num_labels": 2,
      "layers": [
        128
      ],
      "bias": true,
      "act_func": "relu"
    }
  },
  "model_tasks": {
    "task_configs": [
      {
        "name": "binder",
        "keys": [
          "tra",
          "trb",
          "decoder_epitope",
          "decoder_tra",
          "decoder_trb"
        ],
        "target": "binder_label",
        "prefix": "header"
      }
    ],
    "keys": [
      "tra",
      "trb",
      "cdr2a",
      "cdr3b",
      "epitope"
    ]
  },
  "metrics": {
    "binder": {
      "pred_key": "predictions_binder",
      "label_key": "binder_label",
      "metrics": [
        "binary_accuracy",
        "binary_recall",
        "binary_precision",
        "binary_rocauc",
        "binary_f1"
      ],
      "parameters": {
        "threshold": 0.5,
        "norm": true
      }
    }
  },
  "train": {
    "per_device_train_batch_size": 256,
    "num_train_epochs": 500,
    "logging_strategy": "epoch",
    "logging_steps": 5,
    "save_strategy": "epoch",
    "save_steps": 5,
    "remove_unused_columns": false,
    "optim": "adamw_torch",
    "learning_rate": 0.0001,
    "weight_decay": 0,
    "lr_scheduler_type": "cosine",
    "warmup_steps": 10,
    "torch_compile": false,
    "seed": 42,
    "per_device_eval_batch_size": 256,
    "save_n_epoch": 10
  }
}
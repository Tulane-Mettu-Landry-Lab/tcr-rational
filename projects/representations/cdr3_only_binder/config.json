{
  "rules": {
    "0": {
      "rule": "template",
      "template": "{CDR3A}",
      "tokenizer": "aminoacid",
      "prefix": "cdr3a",
      "mapping": {}
    },
    "1": {
      "rule": "template",
      "template": "{CDR3B}",
      "tokenizer": "aminoacid",
      "prefix": "cdr3b",
      "mapping": {}
    },
    "2": {
      "rule": "template",
      "template": "{epitope}",
      "tokenizer": "aminoacid",
      "prefix": "epitope",
      "mapping": {}
    },
    "3": {
      "rule": "forward",
      "keys": [
        "Label"
      ],
      "prefix": null,
      "mapping": {
        "Label": "binder_label"
      }
    }
  },
  "collator": {
    "mlm": true,
    "mlm_probability": 0.5,
    "mask_replace_prob": 1.0,
    "random_replace_prob": 0.0,
    "pad_to_multiple_of": null,
    "seed": null
  },
  "tokenizers": {
    "0": {
      "name": "aminoacid",
      "path": "tokenizers/aminoacid",
      "single": "<CLS> $A <EOS>",
      "pair": "<CLS> $A <SEP> $B:1 <EOS>:1"
    }
  },
  "loss": {
    "header_binder": {
      "mix_weight": 0.5,
      "loss": "cross_entropy",
      "act_func": "softmax",
      "reduction": "mean",
      "label_smoothing": 0.0
    },
    "mlm_cdr3a": {
      "mix_weight": 0.2,
      "loss": "forward",
      "act_func": null
    },
    "mlm_cdr3b": {
      "mix_weight": 0.2,
      "loss": "forward",
      "act_func": null
    },
    "mlm_epitope": {
      "mix_weight": 0.2,
      "loss": "forward",
      "act_func": null
    }
  },
  "model": {
    "cdr3a": {
      "prefix": "cdr3a",
      "model": "BertForMaskedLM",
      "tokenizer": "aminoacid",
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 50,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "cdr3b": {
      "prefix": "cdr3b",
      "model": "BertForMaskedLM",
      "tokenizer": "aminoacid",
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 50,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "epitope": {
      "prefix": "epitope",
      "model": "BertForMaskedLM",
      "tokenizer": "aminoacid",
      "hidden_size": 128,
      "num_hidden_layers": 2,
      "num_attention_heads": 1,
      "intermediate_size": 512,
      "max_position_embeddings": 50,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "position_embedding_type": "absolute",
      "use_cache": true,
      "classifier_dropout": null,
      "attn_implementation": "eager"
    },
    "header_binder": {
      "num_labels": 2,
      "layers": [
        128
      ],
      "bias": true,
      "act_func": "relu"
    }
  },
  "model_tasks": {
    "task_configs": [
      {
        "name": "binder",
        "keys": [
          "cdr3a",
          "cdr3b",
          "epitope"
        ],
        "target": "binder_label",
        "prefix": "header"
      }
    ],
    "keys": [
      "cdr3a",
      "cdr3b",
      "epitope"
    ]
  },
  "metrics": {
    "cdr3a_mlm": {
      "pred_key": "logits_cdr3a",
      "label_key": "cdr3a_labels",
      "metrics": [
        "mlm_rocauc",
        "mlm_recall",
        "mlm_precision",
        "mlm_accuracy"
      ],
      "parameters": {
        "ignore_id": -100,
        "average": "macro",
        "zero_division": 0,
        "multi_class": "ovo",
        "norm": true
      }
    },
    "cdr3b_mlm": {
      "pred_key": "logits_cdr3b",
      "label_key": "cdr3b_labels",
      "metrics": [
        "mlm_rocauc",
        "mlm_recall",
        "mlm_precision",
        "mlm_accuracy"
      ],
      "parameters": {
        "ignore_id": -100,
        "average": "macro",
        "zero_division": 0,
        "multi_class": "ovo",
        "norm": true
      }
    },
    "epitope_mlm": {
      "pred_key": "logits_epitope",
      "label_key": "epitope_labels",
      "metrics": [
        "mlm_rocauc",
        "mlm_recall",
        "mlm_precision",
        "mlm_accuracy"
      ],
      "parameters": {
        "ignore_id": -100,
        "average": "macro",
        "zero_division": 0,
        "multi_class": "ovo",
        "norm": true
      }
    },
    "binder": {
      "pred_key": "predictions_binder",
      "label_key": "binder_label",
      "metrics": [
        "binary_accuracy",
        "binary_recall",
        "binary_precision",
        "binary_rocauc",
        "binary_f1"
      ],
      "parameters": {
        "threshold": 0.5,
        "norm": true
      }
    }
  },
  "train": {
    "dataloader_num_workers": 16,
    "per_device_train_batch_size": 4096,
    "num_train_epochs": 500,
    "logging_strategy": "epoch",
    "logging_steps": 5,
    "save_strategy": "epoch",
    "save_steps": 5,
    "remove_unused_columns": false,
    "optim": "adamw_torch",
    "learning_rate": 0.0001,
    "weight_decay": 0,
    "lr_scheduler_type": "cosine",
    "warmup_steps": 10,
    "torch_compile": false,
    "seed": 42,
    "per_device_eval_batch_size": 4096,
    "save_n_epoch": 10
  }
}